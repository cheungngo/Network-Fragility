{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyOh5guOYBhw1rDym9H3BGZB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# The Original Model"],"metadata":{"id":"hp8x52gNsREW"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wp8doEtiqMgI","executionInfo":{"status":"ok","timestamp":1768061332563,"user_tz":-480,"elapsed":23955,"user":{"displayName":"Ngo Cheung","userId":"02091267041339546959"}},"outputId":"6ccc9984-c7e4-4081-8eb7-52aee3709e66"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","DEVELOPMENTAL PRUNING SIMULATION\n","Modeling synaptic pruning, vulnerability, and plasticity recovery\n","======================================================================\n","\n","[STAGE 1] Training full network (childhood connectivity)...\n","\n","============================================================\n","FULL NETWORK (Pre-pruning baseline)\n","============================================================\n","  Parameters: 396,548 / 396,548 non-zero (0.0% sparse)\n","  Clean test accuracy:     100.0%  (no noise in data)\n","  Standard test accuracy:  100.0%  (noise=0.8 in data)\n","  Noisy test accuracy:     97.8%  (+1.0 input noise)\n","  Very noisy accuracy:     83.6%  (+2.0 input noise)\n","\n","[STAGE 2] Applying aggressive pruning (95% sparsity)...\n","          (Modeling excessive adolescent synaptic elimination)\n","\n","  Per-layer pruning statistics:\n","    net.fc1.weight: kept 52/1024 (94.9% pruned)\n","    net.fc2.weight: kept 13108/262144 (95.0% pruned)\n","    net.fc3.weight: kept 6554/131072 (95.0% pruned)\n","    net.fc4.weight: kept 52/1024 (94.9% pruned)\n","\n","============================================================\n","AFTER AGGRESSIVE PRUNING\n","============================================================\n","  Parameters: 21,050 / 396,548 non-zero (94.7% sparse)\n","  Clean test accuracy:     50.8%  (no noise in data)\n","  Standard test accuracy:  43.1%  (noise=0.8 in data)\n","  Noisy test accuracy:     41.4%  (+1.0 input noise)\n","  Very noisy accuracy:     39.4%  (+2.0 input noise)\n","\n","[STAGE 3] Restoring plasticity (regrowing 50% of pruned connections)...\n","          (Modeling therapeutic neuroplasticity intervention)\n","\n","  Per-layer regrowth statistics:\n","    net.fc1.weight: regrew 486, still pruned 486\n","    net.fc2.weight: regrew 124518, still pruned 124518\n","    net.fc3.weight: regrew 62259, still pruned 62259\n","    net.fc4.weight: regrew 486, still pruned 486\n","\n","  Fine-tuning with regrown connections...\n","\n","============================================================\n","AFTER PLASTICITY RESTORATION\n","============================================================\n","  Parameters: 208,799 / 396,548 non-zero (47.3% sparse)\n","  Clean test accuracy:     100.0%  (no noise in data)\n","  Standard test accuracy:  100.0%  (noise=0.8 in data)\n","  Noisy test accuracy:     97.9%  (+1.0 input noise)\n","  Very noisy accuracy:     84.2%  (+2.0 input noise)\n","\n","======================================================================\n","SUMMARY: Threshold Effects and Recovery\n","======================================================================\n","\n","    Metric              Full    Pruned   Recovered\n","    ─────────────────────────────────────────────────\n","    Clean accuracy      100.0%    50.8%   100.0%\n","    Standard accuracy   100.0%    43.1%   100.0%\n","    Noisy accuracy       97.8%    41.4%    97.9%\n","    Very noisy acc       83.6%    39.4%    84.2%\n","    Sparsity              0.0%    94.7%    47.3%\n","    \n","KEY OBSERVATIONS:\n","  1. Pruning causes larger drops in noisy conditions than clean\n","     → Over-pruned networks lose robustness (psychiatric vulnerability)\n","  2. Recovery is substantial but may not reach full baseline\n","     → Plasticity helps but doesn't fully reverse structural loss\n","  3. The pattern (not just magnitude) of pruning matters\n","     → Per-layer pruning preserves function better than global\n"]}],"source":["\"\"\"\n","Developmental Pruning Simulation: Demonstrating threshold effects and plasticity recovery\n","\n","This script models a simplified analog of adolescent synaptic pruning in neural networks:\n","1. Train an overparameterized network (childhood: dense connectivity)\n","2. Prune aggressively (adolescent pruning: efficiency vs. fragility tradeoff)\n","3. Observe performance collapse, especially under noise (psychiatric vulnerability)\n","4. Regrow connections + fine-tune (therapeutic plasticity restoration)\n","5. Observe recovery (treatment response)\n","\n","Key psychiatric analogs:\n","- Excessive pruning → reduced cognitive flexibility, noise tolerance (schizophrenia prodrome)\n","- Regrowth/plasticity → ketamine-like rapid antidepressant effects, cognitive remediation\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data import DataLoader, TensorDataset\n","from collections import OrderedDict\n","\n","# ============================================================================\n","# REPRODUCIBILITY: Essential for scientific validity of simulation\n","# ============================================================================\n","SEED = 42\n","torch.manual_seed(SEED)\n","np.random.seed(SEED)\n","\n","# Use CPU for reproducibility; CUDA has non-deterministic operations\n","DEVICE = torch.device('cpu')\n","\n","\n","# ============================================================================\n","# DATA GENERATION\n","# ============================================================================\n","def generate_blobs(n_samples: int = 10000, noise: float = 0.8, seed: int = None):\n","    \"\"\"\n","    Generate 4-class classification data: Gaussian blobs at corners of a square.\n","\n","    The noise parameter controls class overlap - higher noise = harder task.\n","    This mimics real-world signal/noise ratios in sensory processing.\n","\n","    Args:\n","        n_samples: Number of data points\n","        noise: Standard deviation of Gaussian noise around cluster centers\n","        seed: Random seed for this specific generation (for separate train/test)\n","\n","    Returns:\n","        Tuple of (features tensor [n_samples, 2], labels tensor [n_samples])\n","    \"\"\"\n","    if seed is not None:\n","        rng = np.random.RandomState(seed)\n","    else:\n","        rng = np.random.RandomState()\n","\n","    # Four well-separated cluster centers\n","    centers = np.array([[-3, -3], [3, 3], [-3, 3], [3, -3]])\n","\n","    # Balanced class distribution\n","    labels = rng.randint(0, 4, n_samples)\n","\n","    # Add Gaussian noise around centers\n","    data = centers[labels] + rng.randn(n_samples, 2) * noise\n","\n","    return (\n","        torch.tensor(data, dtype=torch.float32),\n","        torch.tensor(labels, dtype=torch.long)\n","    )\n","\n","\n","# Generate datasets with DIFFERENT seeds to ensure true generalization test\n","# Using same seed for train/test would create data leakage\n","train_data, train_labels = generate_blobs(12000, noise=0.8, seed=100)\n","test_data, test_labels = generate_blobs(4000, noise=0.8, seed=200)\n","\n","# Clean test set: zero noise, pure signal - tests learned decision boundaries\n","clean_test_data, clean_test_labels = generate_blobs(2000, noise=0.0, seed=300)\n","\n","train_loader = DataLoader(\n","    TensorDataset(train_data, train_labels),\n","    batch_size=128,\n","    shuffle=True\n",")\n","test_loader = DataLoader(\n","    TensorDataset(test_data, test_labels),\n","    batch_size=1000\n",")\n","clean_test_loader = DataLoader(\n","    TensorDataset(clean_test_data, clean_test_labels),\n","    batch_size=1000\n",")\n","\n","\n","# ============================================================================\n","# NETWORK ARCHITECTURE\n","# ============================================================================\n","class Net(nn.Module):\n","    \"\"\"\n","    Deliberately overparameterized network (≈400K parameters for a 2D→4 task).\n","\n","    This excess capacity mirrors the synaptic overgrowth in early childhood.\n","    Pruning will test which connections are truly necessary.\n","\n","    Architecture: 2 → 512 → 512 → 256 → 4\n","    \"\"\"\n","    def __init__(self):\n","        super().__init__()\n","        self.net = nn.Sequential(OrderedDict([\n","            ('fc1', nn.Linear(2, 512)),\n","            ('relu1', nn.ReLU()),\n","            ('fc2', nn.Linear(512, 512)),\n","            ('relu2', nn.ReLU()),\n","            ('fc3', nn.Linear(512, 256)),\n","            ('relu3', nn.ReLU()),\n","            ('fc4', nn.Linear(256, 4))\n","        ]))\n","\n","    def forward(self, x):\n","        return self.net(x)\n","\n","    def count_parameters(self):\n","        \"\"\"Count total and non-zero parameters.\"\"\"\n","        total = sum(p.numel() for p in self.parameters())\n","        nonzero = sum((p != 0).sum().item() for p in self.parameters())\n","        return total, nonzero\n","\n","\n","# ============================================================================\n","# PRUNING INFRASTRUCTURE\n","# ============================================================================\n","class PruningManager:\n","    \"\"\"\n","    Manages weight masks for structured pruning experiments.\n","\n","    Key design choices:\n","    - Per-layer pruning: Prevents small layers from being entirely eliminated\n","    - Mask persistence: Ensures pruned weights stay zero during training\n","    - Tracks pruning history for analysis\n","\n","    Psychiatric analog: This is like tracking which synapses are eliminated\n","    during adolescent pruning - the pattern matters, not just the total count.\n","    \"\"\"\n","\n","    def __init__(self, model: nn.Module):\n","        self.model = model\n","        self.masks = {}  # name → binary mask tensor\n","        self.history = []  # Track pruning/regrowth events\n","\n","        # Initialize masks to all-ones (nothing pruned yet)\n","        for name, param in model.named_parameters():\n","            if 'weight' in name and param.dim() >= 2:  # Only prune weight matrices\n","                self.masks[name] = torch.ones_like(param, dtype=torch.float32)\n","\n","    def prune_by_magnitude(self, sparsity: float, per_layer: bool = True):\n","        \"\"\"\n","        Prune weights by magnitude (smallest weights → zero).\n","\n","        Args:\n","            sparsity: Fraction of weights to prune (0.95 = keep only top 5%)\n","            per_layer: If True, prune each layer independently to target sparsity.\n","                      If False, use global threshold (can eliminate entire layers).\n","\n","        Returns:\n","            Dict with pruning statistics per layer\n","\n","        Biological note: Magnitude-based pruning approximates Hebbian \"use it or lose it\"\n","        since larger weights typically indicate more frequently co-activated pathways.\n","        \"\"\"\n","        stats = {}\n","\n","        if per_layer:\n","            # RECOMMENDED: Prune each layer to target sparsity independently\n","            # This prevents pathological cases where one layer is entirely zeroed\n","            for name, param in self.model.named_parameters():\n","                if name in self.masks:\n","                    weights = param.data.abs()\n","                    threshold = torch.quantile(weights.flatten(), sparsity)\n","\n","                    # Update mask: 1 where weight >= threshold, 0 otherwise\n","                    self.masks[name] = (weights >= threshold).float()\n","\n","                    # Apply mask immediately\n","                    param.data *= self.masks[name]\n","\n","                    # Record statistics\n","                    kept = self.masks[name].sum().item()\n","                    total = self.masks[name].numel()\n","                    stats[name] = {\n","                        'kept': int(kept),\n","                        'total': total,\n","                        'actual_sparsity': 1 - kept/total\n","                    }\n","        else:\n","            # Global threshold: can cause layer collapse, included for comparison\n","            all_weights = torch.cat([\n","                self.model.get_parameter(name).data.abs().flatten()\n","                for name in self.masks\n","            ])\n","            threshold = torch.quantile(all_weights, sparsity)\n","\n","            for name, param in self.model.named_parameters():\n","                if name in self.masks:\n","                    self.masks[name] = (param.data.abs() >= threshold).float()\n","                    param.data *= self.masks[name]\n","\n","                    kept = self.masks[name].sum().item()\n","                    total = self.masks[name].numel()\n","                    stats[name] = {\n","                        'kept': int(kept),\n","                        'total': total,\n","                        'actual_sparsity': 1 - kept/total\n","                    }\n","\n","        self.history.append(('prune', sparsity, stats))\n","        return stats\n","\n","    def regrow_random(self, regrow_fraction: float, init_scale: float = 0.03):\n","        \"\"\"\n","        Randomly regrow a fraction of pruned connections with small initial weights.\n","\n","        Args:\n","            regrow_fraction: Fraction of currently-pruned weights to restore (0.5 = half)\n","            init_scale: Std dev for initializing regrown weights (small = cautious)\n","\n","        Returns:\n","            Dict with regrowth statistics per layer\n","\n","        Psychiatric analog: This models neuroplasticity interventions like:\n","        - Ketamine: Rapid synaptogenesis in prefrontal cortex\n","        - Environmental enrichment: Activity-dependent sprouting\n","        - Cognitive remediation: Strengthening underused pathways\n","\n","        The small initial weights mean regrown connections must \"prove themselves\"\n","        through learning - they're not immediately functional.\n","        \"\"\"\n","        stats = {}\n","\n","        for name, param in self.model.named_parameters():\n","            if name not in self.masks:\n","                continue\n","\n","            # Find currently pruned positions (mask == 0)\n","            pruned_mask = (self.masks[name] == 0)\n","            num_pruned = pruned_mask.sum().item()\n","\n","            if num_pruned == 0:\n","                stats[name] = {'regrown': 0, 'still_pruned': 0}\n","                continue\n","\n","            # Select random subset to regrow\n","            num_regrow = int(regrow_fraction * num_pruned)\n","\n","            if num_regrow == 0:\n","                stats[name] = {'regrown': 0, 'still_pruned': int(num_pruned)}\n","                continue\n","\n","            # Get flat indices of pruned positions\n","            flat_pruned_indices = torch.where(pruned_mask.flatten())[0]\n","\n","            # Randomly select which to regrow\n","            perm = torch.randperm(len(flat_pruned_indices))[:num_regrow]\n","            regrow_indices = flat_pruned_indices[perm]\n","\n","            # Update mask and weights\n","            flat_mask = self.masks[name].flatten()\n","            flat_param = param.data.flatten()\n","\n","            flat_mask[regrow_indices] = 1.0\n","            flat_param[regrow_indices] = torch.randn(num_regrow) * init_scale\n","\n","            # Reshape back (views, so changes persist)\n","            self.masks[name] = flat_mask.view_as(self.masks[name])\n","\n","            stats[name] = {\n","                'regrown': num_regrow,\n","                'still_pruned': int(num_pruned - num_regrow)\n","            }\n","\n","        self.history.append(('regrow', regrow_fraction, stats))\n","        return stats\n","\n","    def apply_masks(self):\n","        \"\"\"\n","        Zero out weights where mask is 0.\n","\n","        CRITICAL: Must be called after each optimizer step during training\n","        to maintain sparsity pattern. Without this, pruned weights drift\n","        back to non-zero values through gradient updates.\n","        \"\"\"\n","        with torch.no_grad():\n","            for name, param in self.model.named_parameters():\n","                if name in self.masks:\n","                    param.data *= self.masks[name]\n","\n","    def get_sparsity(self):\n","        \"\"\"Calculate overall network sparsity.\"\"\"\n","        total_params = 0\n","        zero_params = 0\n","        for name in self.masks:\n","            mask = self.masks[name]\n","            total_params += mask.numel()\n","            zero_params += (mask == 0).sum().item()\n","        return zero_params / total_params if total_params > 0 else 0\n","\n","\n","# ============================================================================\n","# TRAINING AND EVALUATION\n","# ============================================================================\n","def train(\n","    model: nn.Module,\n","    epochs: int = 15,\n","    lr: float = 0.001,\n","    pruning_manager: PruningManager = None,\n","    verbose: bool = False\n","):\n","    \"\"\"\n","    Train model with optional mask enforcement for sparse training.\n","\n","    Args:\n","        model: Network to train\n","        epochs: Training epochs\n","        lr: Learning rate (lower for fine-tuning after regrowth)\n","        pruning_manager: If provided, enforces sparsity masks after each step\n","        verbose: Print loss each epoch\n","\n","    The pruning_manager.apply_masks() call is crucial - without it,\n","    gradient updates would resurrect pruned weights, defeating the pruning.\n","    \"\"\"\n","    optimizer = optim.Adam(model.parameters(), lr=lr)\n","    loss_fn = nn.CrossEntropyLoss()\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        epoch_loss = 0.0\n","\n","        for x, y in train_loader:\n","            x, y = x.to(DEVICE), y.to(DEVICE)\n","\n","            optimizer.zero_grad()\n","            loss = loss_fn(model(x), y)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # CRITICAL: Re-apply masks after each optimizer step\n","            # This maintains the sparsity structure\n","            if pruning_manager is not None:\n","                pruning_manager.apply_masks()\n","\n","            epoch_loss += loss.item()\n","\n","        if verbose:\n","            print(f\"  Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}\")\n","\n","\n","def evaluate(model: nn.Module, loader: DataLoader, noise_std: float = 0.0):\n","    \"\"\"\n","    Evaluate model accuracy, optionally with added input noise.\n","\n","    Args:\n","        model: Network to evaluate\n","        loader: DataLoader with test data\n","        noise_std: Standard deviation of Gaussian noise to add to inputs\n","                  (tests robustness to perturbations)\n","\n","    Returns:\n","        Accuracy as percentage\n","\n","    Psychiatric note: Noise tolerance is a key measure of network robustness.\n","    Over-pruned networks lose their \"margin of safety\" and misclassify\n","    under perturbation - analogous to cognitive instability under stress.\n","    \"\"\"\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for x, y in loader:\n","            x, y = x.to(DEVICE), y.to(DEVICE)\n","\n","            # Add noise to test robustness\n","            if noise_std > 0:\n","                x = x + torch.randn_like(x) * noise_std\n","\n","            predictions = model(x).argmax(dim=1)\n","            correct += (predictions == y).sum().item()\n","            total += y.size(0)\n","\n","    return 100.0 * correct / total\n","\n","\n","def comprehensive_eval(model: nn.Module, label: str):\n","    \"\"\"Run full evaluation suite and print results.\"\"\"\n","    clean_acc = evaluate(model, clean_test_loader, noise_std=0.0)\n","    standard_acc = evaluate(model, test_loader, noise_std=0.0)\n","    noisy_acc = evaluate(model, test_loader, noise_std=1.0)\n","    very_noisy_acc = evaluate(model, test_loader, noise_std=2.0)\n","\n","    total, nonzero = model.count_parameters()\n","    sparsity = 100 * (1 - nonzero / total)\n","\n","    print(f\"\\n{'='*60}\")\n","    print(f\"{label}\")\n","    print(f\"{'='*60}\")\n","    print(f\"  Parameters: {nonzero:,} / {total:,} non-zero ({sparsity:.1f}% sparse)\")\n","    print(f\"  Clean test accuracy:     {clean_acc:.1f}%  (no noise in data)\")\n","    print(f\"  Standard test accuracy:  {standard_acc:.1f}%  (noise=0.8 in data)\")\n","    print(f\"  Noisy test accuracy:     {noisy_acc:.1f}%  (+1.0 input noise)\")\n","    print(f\"  Very noisy accuracy:     {very_noisy_acc:.1f}%  (+2.0 input noise)\")\n","\n","    return {\n","        'clean': clean_acc,\n","        'standard': standard_acc,\n","        'noisy': noisy_acc,\n","        'very_noisy': very_noisy_acc,\n","        'sparsity': sparsity\n","    }\n","\n","\n","# ============================================================================\n","# MAIN EXPERIMENT\n","# ============================================================================\n","def run_experiment():\n","    \"\"\"\n","    Full experimental pipeline demonstrating the pruning-plasticity hypothesis.\n","\n","    Stages:\n","    1. BASELINE: Train overparameterized network (childhood connectivity)\n","    2. PRUNING: Remove 95% of weights by magnitude (adolescent pruning)\n","    3. OBSERVATION: Note performance drop, especially under noise\n","    4. PLASTICITY: Regrow 50% of pruned connections + fine-tune\n","    5. RECOVERY: Observe performance restoration\n","\n","    This models the psychiatric hypothesis that:\n","    - Normal pruning → efficiency gains\n","    - Excessive pruning → vulnerability to noise/stress (prodromal symptoms)\n","    - Plasticity restoration → therapeutic recovery\n","    \"\"\"\n","\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"DEVELOPMENTAL PRUNING SIMULATION\")\n","    print(\"Modeling synaptic pruning, vulnerability, and plasticity recovery\")\n","    print(\"=\"*70)\n","\n","    results = {}\n","\n","    # ========================================================================\n","    # STAGE 1: Train full network (childhood - dense connectivity)\n","    # ========================================================================\n","    print(\"\\n[STAGE 1] Training full network (childhood connectivity)...\")\n","\n","    model = Net().to(DEVICE)\n","    train(model, epochs=20, lr=0.001)\n","    results['full'] = comprehensive_eval(model, \"FULL NETWORK (Pre-pruning baseline)\")\n","\n","    # ========================================================================\n","    # STAGE 2: Aggressive pruning (adolescent synaptic elimination)\n","    # ========================================================================\n","    print(\"\\n[STAGE 2] Applying aggressive pruning (95% sparsity)...\")\n","    print(\"          (Modeling excessive adolescent synaptic elimination)\")\n","\n","    pruning_mgr = PruningManager(model)\n","    prune_stats = pruning_mgr.prune_by_magnitude(sparsity=0.95, per_layer=True)\n","\n","    print(\"\\n  Per-layer pruning statistics:\")\n","    for name, stats in prune_stats.items():\n","        print(f\"    {name}: kept {stats['kept']}/{stats['total']} \"\n","              f\"({100*stats['actual_sparsity']:.1f}% pruned)\")\n","\n","    results['pruned'] = comprehensive_eval(model, \"AFTER AGGRESSIVE PRUNING\")\n","\n","    # ========================================================================\n","    # STAGE 3: Plasticity restoration (therapeutic intervention)\n","    # ========================================================================\n","    print(\"\\n[STAGE 3] Restoring plasticity (regrowing 50% of pruned connections)...\")\n","    print(\"          (Modeling therapeutic neuroplasticity intervention)\")\n","\n","    regrow_stats = pruning_mgr.regrow_random(regrow_fraction=0.5, init_scale=0.03)\n","\n","    print(\"\\n  Per-layer regrowth statistics:\")\n","    for name, stats in regrow_stats.items():\n","        print(f\"    {name}: regrew {stats['regrown']}, \"\n","              f\"still pruned {stats['still_pruned']}\")\n","\n","    # Fine-tune with lower learning rate (careful retraining of new connections)\n","    print(\"\\n  Fine-tuning with regrown connections...\")\n","    train(model, epochs=15, lr=0.0005, pruning_manager=pruning_mgr)\n","\n","    results['recovered'] = comprehensive_eval(model, \"AFTER PLASTICITY RESTORATION\")\n","\n","    # ========================================================================\n","    # SUMMARY\n","    # ========================================================================\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"SUMMARY: Threshold Effects and Recovery\")\n","    print(\"=\"*70)\n","\n","    print(f\"\"\"\n","    Metric              Full    Pruned   Recovered\n","    ─────────────────────────────────────────────────\n","    Clean accuracy      {results['full']['clean']:5.1f}%   {results['pruned']['clean']:5.1f}%   {results['recovered']['clean']:5.1f}%\n","    Standard accuracy   {results['full']['standard']:5.1f}%   {results['pruned']['standard']:5.1f}%   {results['recovered']['standard']:5.1f}%\n","    Noisy accuracy      {results['full']['noisy']:5.1f}%   {results['pruned']['noisy']:5.1f}%   {results['recovered']['noisy']:5.1f}%\n","    Very noisy acc      {results['full']['very_noisy']:5.1f}%   {results['pruned']['very_noisy']:5.1f}%   {results['recovered']['very_noisy']:5.1f}%\n","    Sparsity            {results['full']['sparsity']:5.1f}%   {results['pruned']['sparsity']:5.1f}%   {results['recovered']['sparsity']:5.1f}%\n","    \"\"\")\n","\n","    print(\"KEY OBSERVATIONS:\")\n","    print(\"  1. Pruning causes larger drops in noisy conditions than clean\")\n","    print(\"     → Over-pruned networks lose robustness (psychiatric vulnerability)\")\n","    print(\"  2. Recovery is substantial but may not reach full baseline\")\n","    print(\"     → Plasticity helps but doesn't fully reverse structural loss\")\n","    print(\"  3. The pattern (not just magnitude) of pruning matters\")\n","    print(\"     → Per-layer pruning preserves function better than global\")\n","\n","    return results\n","\n","\n","# ============================================================================\n","# SPARSITY SWEEP: Finding the critical threshold\n","# ============================================================================\n","def sparsity_sweep():\n","    \"\"\"\n","    Test multiple sparsity levels to find the critical threshold.\n","\n","    This models the question: \"How much pruning is too much?\"\n","\n","    Expect to see:\n","    - Low sparsity (0-70%): Minimal performance loss\n","    - Medium (70-90%): Gradual degradation\n","    - High (90-97%): Rapid collapse (threshold effect)\n","    - Extreme (>97%): Near-chance performance\n","\n","    The threshold effect is the key finding - there's a \"cliff\" where\n","    the network suddenly loses its ability to generalize.\n","    \"\"\"\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"SPARSITY SWEEP: Finding the critical pruning threshold\")\n","    print(\"=\"*70)\n","\n","    sparsity_levels = [0.0, 0.5, 0.7, 0.8, 0.9, 0.95, 0.97, 0.99]\n","\n","    print(f\"\\n{'Sparsity':>10} {'Clean':>10} {'Standard':>10} {'Noisy':>10}\")\n","    print(\"-\" * 45)\n","\n","    for sparsity in sparsity_levels:\n","        # Fresh model for each sparsity level\n","        model = Net().to(DEVICE)\n","        train(model, epochs=20, lr=0.001)\n","\n","        if sparsity > 0:\n","            pruning_mgr = PruningManager(model)\n","            pruning_mgr.prune_by_magnitude(sparsity=sparsity, per_layer=True)\n","\n","        clean = evaluate(model, clean_test_loader)\n","        standard = evaluate(model, test_loader)\n","        noisy = evaluate(model, test_loader, noise_std=1.0)\n","\n","        print(f\"{sparsity*100:>9.0f}% {clean:>9.1f}% {standard:>9.1f}% {noisy:>9.1f}%\")\n","\n","    print(\"\\nNote: Look for the 'cliff' - where performance drops sharply.\")\n","    print(\"This threshold varies by task complexity and network architecture.\")\n","\n","\n","if __name__ == \"__main__\":\n","    # Run main experiment\n","    results = run_experiment()\n","\n","    # Optionally run sparsity sweep (uncomment to explore thresholds)\n","    # sparsity_sweep()"]},{"cell_type":"markdown","source":["# The End"],"metadata":{"id":"8E-hV0TvCf79"}}]}